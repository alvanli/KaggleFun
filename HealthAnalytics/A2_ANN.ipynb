{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "sOCSeNfQcUM1",
    "outputId": "01177b6d-0299-418f-d81a-c53a3f055354"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hospital_type_code</th>\n",
       "      <th>City_Code_Hospital</th>\n",
       "      <th>Hospital_region_code</th>\n",
       "      <th>Available Extra Rooms in Hospital</th>\n",
       "      <th>Department</th>\n",
       "      <th>Ward_Type</th>\n",
       "      <th>Ward_Facility_Code</th>\n",
       "      <th>Bed Grade</th>\n",
       "      <th>City_Code_Patient</th>\n",
       "      <th>Type of Admission</th>\n",
       "      <th>Severity of Illness</th>\n",
       "      <th>Visitors with Patient</th>\n",
       "      <th>Age</th>\n",
       "      <th>Admission_Deposit</th>\n",
       "      <th>Stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231676</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6247.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166821</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70566</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4987.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197982</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7210.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>280389</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3178.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Hospital_type_code  City_Code_Hospital  Hospital_region_code  \\\n",
       "0      231676                   0                   7                     1   \n",
       "1      166821                   0                   7                     1   \n",
       "2       70566                   1                   2                     1   \n",
       "3      197982                   1                   2                     1   \n",
       "4      280389                   3                  13                     1   \n",
       "\n",
       "   Available Extra Rooms in Hospital  Department  Ward_Type  \\\n",
       "0                                  4           2          1   \n",
       "1                                  2           3          3   \n",
       "2                                  2           3          2   \n",
       "3                                  2           2          1   \n",
       "4                                  4           3          2   \n",
       "\n",
       "   Ward_Facility_Code  Bed Grade  City_Code_Patient  Type of Admission  \\\n",
       "0                   2        2.0                8.0                  0   \n",
       "1                   2        1.0                2.0                  0   \n",
       "2                   3        3.0                7.0                  0   \n",
       "3                   3        3.0               14.0                  1   \n",
       "4                   1        2.0                8.0                  2   \n",
       "\n",
       "   Severity of Illness  Visitors with Patient  Age  Admission_Deposit  Stay  \n",
       "0                    1                      4    4             6247.0     2  \n",
       "1                    1                      6    6             8000.0    10  \n",
       "2                    2                      3    2             4987.0     1  \n",
       "3                    1                      3    6             7210.0     6  \n",
       "4                    1                      2    4             3178.0     5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "base_dir = \"/home/aldec/data/random_stuff/KaggleFun/HealthAnalytics\"\n",
    "df_train = pd.read_csv(os.path.join(base_dir,\"pro_train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(base_dir,\"pro_test.csv\"))\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wha9HzDHcUM8",
    "outputId": "d647a1af-2b2c-4564-9dab-f4ff5eb9ee12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "cats = [\"Hospital_type_code\",\"City_Code_Hospital\",\"Hospital_region_code\",\"Department\",\"City_Code_Patient\", \"Ward_Type\", \"Ward_Facility_Code\", \"Type of Admission\"]\n",
    "nums = [\"Severity of Illness\", \"Age\", \"Bed Grade\", \"Admission_Deposit\", \"Visitors with Patient\"]\n",
    "print(len(cats), len(nums))\n",
    "print(len(df_train.columns))\n",
    "# stay and unnamed are dropped, 16 - 2 = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JhFGCs-ycUM_",
    "outputId": "ed2e9bf4-1084-4890-fc68-0dba1582429f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(df_train.copy().drop(columns=cats+[\"Stay\",\"Unnamed: 0\"]))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "oh = OneHotEncoder()\n",
    "oh.fit(df_train.Stay.to_numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pWPBEKNxcUNB",
    "outputId": "1602ce36-b3cb-4acc-98da-b0c5e8865710"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 4), (14, 6), (3, 2), (5, 3), (39, 19), (6, 3), (6, 3), (3, 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes = [(int(max(df_train[cat].unique())+1), min(50, (len(df_train[cat].unique())+1)//2)) for cat in cats]\n",
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3StKVrUscUND"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop([\"Unnamed: 0\"], axis = 1)\n",
    "df_test = df_test.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4NIVxTOcUNJ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, df, cats):\n",
    "        self.x_cat = df.loc[:,cats].copy().values.astype(np.int64) #categorical columns\n",
    "        self.x_num = sc.transform(df.drop(columns=cats+[\"Stay\"]).copy()).astype(np.float32) #numerical columns\n",
    "        self.y = oh.transform(df.Stay.to_numpy().reshape(-1,1)).toarray()\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_cat[idx], self.x_num[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWaW0C78cUNM"
   },
   "outputs": [],
   "source": [
    "train_data = TabDataset(df_train, cats)\n",
    "test_data = TabDataset(df_test, cats)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last = True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y54kZ2fccUNP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.n_cont = n_cont\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.linearBlock = nn.Sequential(\n",
    "            nn.Linear(self.n_emb + self.n_cont, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64,11)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x_1 = [em(x1[:,i]) for i, em in enumerate(self.embeddings)]\n",
    "            \n",
    "        x_1 = torch.cat(x_1, axis = 1)\n",
    "        x_1 = self.emb_drop(x_1)\n",
    "        x_2 = self.bn1(x2)\n",
    "        \n",
    "        x = torch.cat([x_1, x_2], axis = 1)\n",
    "        y = self.linearBlock(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "id": "PdJ7WgdTcUNR",
    "outputId": "887dd32f-d725-41d4-c92e-8f9f6b35ac7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "ANN(\n",
      "  (embeddings): ModuleList(\n",
      "    (0): Embedding(7, 4)\n",
      "    (1): Embedding(14, 6)\n",
      "    (2): Embedding(3, 2)\n",
      "    (3): Embedding(5, 3)\n",
      "    (4): Embedding(39, 19)\n",
      "    (5): Embedding(6, 3)\n",
      "    (6): Embedding(6, 3)\n",
      "    (7): Embedding(3, 2)\n",
      "  )\n",
      "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
      "  (bn1): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linearBlock): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Dropout(p=0.4, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Dropout(p=0.4, inplace=False)\n",
      "    (9): Linear(in_features=64, out_features=11, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = ANN(embedding_sizes, 6)\n",
    "model.to(device)\n",
    "lr=0.05\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 100\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46vLqRZNcUNT"
   },
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)   \n",
    "    y_test_num = torch.argmax(y_test, axis=1)\n",
    "    correct_pred = (y_pred_tags == y_test_num).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    return acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_acc = 0\n",
    "loss_min = np.inf\n",
    "acc_epoch = []\n",
    "test_acc_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lpBJ7u_fcUNV",
    "outputId": "7d482199-a277-4100-bc79-fa2d6b890537",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Acc: 9.00, Val Acc: 13.50\n",
      "Epoch: 1, Acc: 26.00, Val Acc: 37.49\n",
      "Epoch: 9, Acc: 31.00, Val Acc: 37.81\n",
      "Epoch: 12, Acc: 36.00, Val Acc: 38.42\n",
      "Epoch: 59, Acc: 39.00, Val Acc: 38.57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-93e7f6b044dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b8befc26c73c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinearBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2150\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    for batch_idx, (x1, x2, y) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(x1, x2)\n",
    "        loss = criterion(output, torch.argmax(y, axis=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if batch_idx == 0:\n",
    "                test_acc = []\n",
    "                for xx1, xx2, yy in test_loader:\n",
    "                    xx1, xx2, yy = xx1.to(device), xx2.to(device), yy.to(device)\n",
    "                    model.zero_grad()\n",
    "                    yyout = model(xx1, xx2)\n",
    "                    test_acc.append(multi_acc(yyout, yy).detach().cpu().numpy())\n",
    "                acc_epoch.append(multi_acc(output, y))\n",
    "                test_acc_epoch.append(np.mean(test_acc))\n",
    "                if np.mean(test_acc) > best_test_acc:\n",
    "                    print(\"Epoch: {}, Acc: {:.2f}, Val Acc: {:.2f}\".format(i, multi_acc(output, y), np.mean(test_acc)))\n",
    "                    best_test_acc = np.mean(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use roc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(100),acc_epoch)\n",
    "plt.plot(range(100),test_acc_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        lr=0.05,\n",
    "        epochs=200,\n",
    "        batch_size=100,\n",
    "        model=[ \n",
    "            emb_drop = nn.Dropout(0.6),\n",
    "            nn.Linear(self.n_emb + self.n_cont, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64,11)\n",
    "        ],\n",
    "        em_sizes=[(7, 4), (14, 6), (3, 2), (5, 3), (39, 19), (6, 3), (6, 3), (3, 2)],\n",
    "        acc=39,\n",
    "        test_acc=38.9,\n",
    "    },\n",
    "    {\n",
    "        lr=0.03,\n",
    "        epochs=200,\n",
    "        batch_size=100,\n",
    "        model=[ \n",
    "            emb_drop = nn.Dropout(0.3),\n",
    "            nn.Linear(self.n_emb + self.n_cont, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256,256),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(256,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64,11)\n",
    "        ],\n",
    "        em_sizes=[(7, 4), (14, 6), (3, 2), (5, 3), (39, 19), (6, 3), (6, 3), (3, 2)],\n",
    "        acc=39,\n",
    "        test_acc=38.9,\n",
    "    },\n",
    "    {\n",
    "        lr=0.03,\n",
    "        epochs=200,\n",
    "        batch_size=100,\n",
    "        model=[ \n",
    "            emb_drop = nn.Dropout(0.3),\n",
    "            nn.Linear(self.n_emb + self.n_cont, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512,11)\n",
    "        ],\n",
    "        em_sizes=[(7, 4), (14, 6), (3, 2), (5, 3), (39, 19), (6, 3), (6, 3), (3, 2)],\n",
    "        acc=42,\n",
    "        test_acc=40,\n",
    "    },\n",
    "    {\n",
    "        lr=0.03,\n",
    "        epochs=200,\n",
    "        batch_size=100,\n",
    "        model=[ \n",
    "            emb_drop = nn.Dropout(0.3),\n",
    "            nn.Linear(self.n_emb + self.n_cont, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256,512),\n",
    "            nn.Linear(512,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024,11)\n",
    "        ],\n",
    "        em_sizes=[(7, 4), (14, 6), (3, 2), (5, 3), (39, 19), (6, 3), (6, 3), (3, 2)],\n",
    "        acc=37,\n",
    "        test_acc=39,\n",
    "    },\n",
    "     {\n",
    "        lr=0.07,\n",
    "        epochs=200,\n",
    "        batch_size=1000,\n",
    "        model=[ \n",
    "            emb_drop = nn.Dropout(0.3),\n",
    "            nn.Linear(self.n_emb + self.n_cont, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256,512),\n",
    "            nn.Linear(512,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024,11)\n",
    "        ],\n",
    "        em_sizes=[(7, 4), (14, 6), (3, 2), (5, 3), (39, 19), (6, 3), (6, 3), (3, 2)],\n",
    "        acc=37,\n",
    "        test_acc=39,\n",
    "    }\n",
    "\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A1_ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
