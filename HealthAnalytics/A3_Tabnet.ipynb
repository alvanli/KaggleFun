{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_Tabnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "sOCSeNfQcUM1",
        "outputId": "7696a734-1aa3-4841-91cf-8b0773c2c735"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "base_dir = \"\"\n",
        "df_train = pd.read_csv(os.path.join(base_dir,\"pro_train.csv\"))\n",
        "df_test = pd.read_csv(os.path.join(base_dir,\"pro_test.csv\"))\n",
        "df_train.head(5)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Hospital_type_code</th>\n",
              "      <th>City_Code_Hospital</th>\n",
              "      <th>Hospital_region_code</th>\n",
              "      <th>Available Extra Rooms in Hospital</th>\n",
              "      <th>Department</th>\n",
              "      <th>Ward_Type</th>\n",
              "      <th>Ward_Facility_Code</th>\n",
              "      <th>Bed Grade</th>\n",
              "      <th>City_Code_Patient</th>\n",
              "      <th>Type of Admission</th>\n",
              "      <th>Severity of Illness</th>\n",
              "      <th>Visitors with Patient</th>\n",
              "      <th>Age</th>\n",
              "      <th>Admission_Deposit</th>\n",
              "      <th>Stay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>231676</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6247.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>166821</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>8000.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70566</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4987.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>197982</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>7210.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>280389</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3178.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  Hospital_type_code  ...  Admission_Deposit  Stay\n",
              "0      231676                   0  ...             6247.0     2\n",
              "1      166821                   0  ...             8000.0    10\n",
              "2       70566                   1  ...             4987.0     1\n",
              "3      197982                   1  ...             7210.0     6\n",
              "4      280389                   3  ...             3178.0     5\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3StKVrUscUND"
      },
      "source": [
        "df_train = df_train.drop([\"Unnamed: 0\"], axis = 1)\n",
        "df_test = df_test.drop([\"Unnamed: 0\"], axis=1)\n",
        "\n",
        "df_y = df_train.Stay\n",
        "df_train = df_train.drop(\"Stay\", axis = 1).dropna()\n",
        "\n",
        "df_test_y = df_test.Stay\n",
        "df_test = df_test.drop(\"Stay\", axis=1).dropna()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wha9HzDHcUM8"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_attribs = [\"Available Extra Rooms in Hospital\", \"Bed Grade\", \"Visitors with Patient\", \"Admission_Deposit\"]\n",
        "cat_attribs = [\"Hospital_type_code\", \"City_Code_Hospital\", \"Hospital_region_code\", \"Department\", \"Ward_Type\", \"Ward_Facility_Code\", \"City_Code_Patient\", \"Type of Admission\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_train[num_attribs] = scaler.fit_transform(df_train[num_attribs])\n",
        "df_test[num_attribs] = scaler.fit_transform(df_test[num_attribs])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDhvOkvQ1-cU",
        "outputId": "e1205e56-41b3-483f-9c30-798b40c6acba"
      },
      "source": [
        "%pip install pytorch-tabnet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading https://files.pythonhosted.org/packages/94/e5/2a808d611a5d44e3c997c0d07362c04a56c70002208e00aec9eee3d923b5/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.8.1+cu101)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.41.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.0.1)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y54kZ2fccUNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9934c0-8511-4c0b-f876-89cfd2f1e090"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "cat_idxs = [ i for i, f in enumerate(df_train.columns) if f in cat_attribs]\n",
        "cat_dims = [len((df_train[cat]+df_test[cat]).unique()) for cat in cat_attribs]\n",
        "\n",
        "# https://pypi.org/project/pytorch-tabnet/\n",
        "model = TabNetClassifier(n_d=12, n_a=12, cat_idxs=cat_idxs, cat_dims=cat_dims, n_steps=5)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device used : cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjR3YF5-deeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2774e33-62cb-4978-d8fc-78d153ac03e4"
      },
      "source": [
        "model.fit(\n",
        "    X_train=df_train.values, y_train=df_y.values,\n",
        "    eval_set=[(df_train.values, df_y.values), (df_test.values, df_test_y.values)],\n",
        "    eval_metric=[\"accuracy\"],\n",
        "    eval_name=[\"train\", \"test\"],\n",
        "    max_epochs=200, patience=100,\n",
        "    batch_size=5000, virtual_batch_size=256\n",
        ")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 2.05666 | train_accuracy: 0.33511 | test_accuracy: 0.33524 |  0:00:38s\n",
            "epoch 1  | loss: 1.71435 | train_accuracy: 0.34915 | test_accuracy: 0.34854 |  0:01:16s\n",
            "epoch 2  | loss: 1.68351 | train_accuracy: 0.35816 | test_accuracy: 0.35701 |  0:01:54s\n",
            "epoch 3  | loss: 1.66891 | train_accuracy: 0.36431 | test_accuracy: 0.36366 |  0:02:35s\n",
            "epoch 4  | loss: 1.65839 | train_accuracy: 0.37041 | test_accuracy: 0.37118 |  0:03:17s\n",
            "epoch 5  | loss: 1.65044 | train_accuracy: 0.37248 | test_accuracy: 0.37305 |  0:03:56s\n",
            "epoch 6  | loss: 1.64993 | train_accuracy: 0.37521 | test_accuracy: 0.37608 |  0:04:36s\n",
            "epoch 7  | loss: 1.63936 | train_accuracy: 0.37675 | test_accuracy: 0.37801 |  0:05:12s\n",
            "epoch 8  | loss: 1.63424 | train_accuracy: 0.38091 | test_accuracy: 0.37995 |  0:05:49s\n",
            "epoch 9  | loss: 1.62227 | train_accuracy: 0.39002 | test_accuracy: 0.39224 |  0:06:27s\n",
            "epoch 10 | loss: 1.5952  | train_accuracy: 0.39549 | test_accuracy: 0.39406 |  0:07:07s\n",
            "epoch 11 | loss: 1.58546 | train_accuracy: 0.39581 | test_accuracy: 0.39571 |  0:07:54s\n",
            "epoch 12 | loss: 1.58034 | train_accuracy: 0.40224 | test_accuracy: 0.40091 |  0:08:41s\n",
            "epoch 13 | loss: 1.57381 | train_accuracy: 0.40149 | test_accuracy: 0.4007  |  0:09:22s\n",
            "epoch 14 | loss: 1.57049 | train_accuracy: 0.4016  | test_accuracy: 0.40152 |  0:10:00s\n",
            "epoch 15 | loss: 1.57032 | train_accuracy: 0.40177 | test_accuracy: 0.40276 |  0:10:39s\n",
            "epoch 16 | loss: 1.56764 | train_accuracy: 0.40222 | test_accuracy: 0.40256 |  0:11:19s\n",
            "epoch 17 | loss: 1.56412 | train_accuracy: 0.40533 | test_accuracy: 0.40658 |  0:11:56s\n",
            "epoch 18 | loss: 1.56399 | train_accuracy: 0.40326 | test_accuracy: 0.40315 |  0:12:35s\n",
            "epoch 19 | loss: 1.56738 | train_accuracy: 0.40489 | test_accuracy: 0.40419 |  0:13:13s\n",
            "epoch 20 | loss: 1.56751 | train_accuracy: 0.39905 | test_accuracy: 0.39868 |  0:13:52s\n",
            "epoch 21 | loss: 1.56428 | train_accuracy: 0.40567 | test_accuracy: 0.40479 |  0:14:32s\n",
            "epoch 22 | loss: 1.56014 | train_accuracy: 0.40721 | test_accuracy: 0.40868 |  0:15:13s\n",
            "epoch 23 | loss: 1.55581 | train_accuracy: 0.40475 | test_accuracy: 0.40474 |  0:15:52s\n",
            "epoch 24 | loss: 1.55564 | train_accuracy: 0.40669 | test_accuracy: 0.40741 |  0:16:36s\n",
            "epoch 25 | loss: 1.55572 | train_accuracy: 0.40564 | test_accuracy: 0.40606 |  0:17:15s\n",
            "epoch 26 | loss: 1.55352 | train_accuracy: 0.40754 | test_accuracy: 0.40797 |  0:17:55s\n",
            "epoch 27 | loss: 1.55205 | train_accuracy: 0.40908 | test_accuracy: 0.4084  |  0:18:34s\n",
            "epoch 28 | loss: 1.55219 | train_accuracy: 0.40918 | test_accuracy: 0.40973 |  0:19:12s\n",
            "epoch 29 | loss: 1.55176 | train_accuracy: 0.40915 | test_accuracy: 0.40925 |  0:19:50s\n",
            "epoch 30 | loss: 1.54994 | train_accuracy: 0.40939 | test_accuracy: 0.40946 |  0:20:29s\n",
            "epoch 31 | loss: 1.54912 | train_accuracy: 0.41054 | test_accuracy: 0.41104 |  0:21:09s\n",
            "epoch 32 | loss: 1.54772 | train_accuracy: 0.41105 | test_accuracy: 0.41231 |  0:21:50s\n",
            "epoch 33 | loss: 1.54652 | train_accuracy: 0.411   | test_accuracy: 0.4105  |  0:22:30s\n",
            "epoch 34 | loss: 1.54682 | train_accuracy: 0.41026 | test_accuracy: 0.40871 |  0:23:08s\n",
            "epoch 35 | loss: 1.54747 | train_accuracy: 0.41093 | test_accuracy: 0.41072 |  0:23:48s\n",
            "epoch 36 | loss: 1.54588 | train_accuracy: 0.40971 | test_accuracy: 0.41072 |  0:24:27s\n",
            "epoch 37 | loss: 1.54405 | train_accuracy: 0.41132 | test_accuracy: 0.4118  |  0:25:13s\n",
            "epoch 38 | loss: 1.54474 | train_accuracy: 0.41134 | test_accuracy: 0.41256 |  0:25:56s\n",
            "epoch 39 | loss: 1.54495 | train_accuracy: 0.41113 | test_accuracy: 0.41108 |  0:26:34s\n",
            "epoch 40 | loss: 1.55387 | train_accuracy: 0.392   | test_accuracy: 0.39171 |  0:27:11s\n",
            "epoch 41 | loss: 1.552   | train_accuracy: 0.41252 | test_accuracy: 0.4133  |  0:27:48s\n",
            "epoch 42 | loss: 1.5465  | train_accuracy: 0.41136 | test_accuracy: 0.41204 |  0:28:24s\n",
            "epoch 43 | loss: 1.54405 | train_accuracy: 0.4108  | test_accuracy: 0.41078 |  0:29:00s\n",
            "epoch 44 | loss: 1.55149 | train_accuracy: 0.40926 | test_accuracy: 0.40821 |  0:29:37s\n",
            "epoch 45 | loss: 1.54943 | train_accuracy: 0.41161 | test_accuracy: 0.41094 |  0:30:13s\n",
            "epoch 46 | loss: 1.5449  | train_accuracy: 0.41342 | test_accuracy: 0.4129  |  0:30:50s\n",
            "epoch 47 | loss: 1.54499 | train_accuracy: 0.41205 | test_accuracy: 0.41053 |  0:31:26s\n",
            "epoch 48 | loss: 1.54565 | train_accuracy: 0.41242 | test_accuracy: 0.4125  |  0:32:03s\n",
            "epoch 49 | loss: 1.54616 | train_accuracy: 0.41155 | test_accuracy: 0.4113  |  0:32:39s\n",
            "epoch 50 | loss: 1.545   | train_accuracy: 0.40858 | test_accuracy: 0.40763 |  0:33:16s\n",
            "epoch 51 | loss: 1.54555 | train_accuracy: 0.41125 | test_accuracy: 0.40942 |  0:33:53s\n",
            "epoch 52 | loss: 1.54223 | train_accuracy: 0.40998 | test_accuracy: 0.40885 |  0:34:31s\n",
            "epoch 53 | loss: 1.54318 | train_accuracy: 0.41235 | test_accuracy: 0.411   |  0:35:07s\n",
            "epoch 54 | loss: 1.54106 | train_accuracy: 0.4124  | test_accuracy: 0.41044 |  0:35:44s\n",
            "epoch 55 | loss: 1.53822 | train_accuracy: 0.41379 | test_accuracy: 0.4116  |  0:36:20s\n",
            "epoch 56 | loss: 1.53696 | train_accuracy: 0.41414 | test_accuracy: 0.41314 |  0:36:58s\n",
            "epoch 57 | loss: 1.53623 | train_accuracy: 0.41392 | test_accuracy: 0.41237 |  0:37:35s\n",
            "epoch 58 | loss: 1.53706 | train_accuracy: 0.41378 | test_accuracy: 0.41213 |  0:38:11s\n",
            "epoch 59 | loss: 1.53589 | train_accuracy: 0.41323 | test_accuracy: 0.41184 |  0:38:48s\n",
            "epoch 60 | loss: 1.53584 | train_accuracy: 0.41278 | test_accuracy: 0.41198 |  0:39:25s\n",
            "epoch 61 | loss: 1.53413 | train_accuracy: 0.41484 | test_accuracy: 0.41348 |  0:40:01s\n",
            "epoch 62 | loss: 1.53271 | train_accuracy: 0.41374 | test_accuracy: 0.41184 |  0:40:38s\n",
            "epoch 63 | loss: 1.53322 | train_accuracy: 0.41364 | test_accuracy: 0.41231 |  0:41:16s\n",
            "epoch 64 | loss: 1.53195 | train_accuracy: 0.41434 | test_accuracy: 0.41466 |  0:41:53s\n",
            "epoch 65 | loss: 1.53219 | train_accuracy: 0.41527 | test_accuracy: 0.41403 |  0:42:30s\n",
            "epoch 66 | loss: 1.53116 | train_accuracy: 0.41252 | test_accuracy: 0.41231 |  0:43:07s\n",
            "epoch 67 | loss: 1.53169 | train_accuracy: 0.41381 | test_accuracy: 0.41268 |  0:43:43s\n",
            "epoch 68 | loss: 1.53044 | train_accuracy: 0.41455 | test_accuracy: 0.41242 |  0:44:21s\n",
            "epoch 69 | loss: 1.53334 | train_accuracy: 0.41478 | test_accuracy: 0.41304 |  0:44:59s\n",
            "epoch 70 | loss: 1.53136 | train_accuracy: 0.4147  | test_accuracy: 0.41449 |  0:45:37s\n",
            "epoch 71 | loss: 1.53203 | train_accuracy: 0.41428 | test_accuracy: 0.41042 |  0:46:13s\n",
            "epoch 72 | loss: 1.52959 | train_accuracy: 0.41551 | test_accuracy: 0.41375 |  0:46:50s\n",
            "epoch 73 | loss: 1.52892 | train_accuracy: 0.41503 | test_accuracy: 0.41414 |  0:47:27s\n",
            "epoch 74 | loss: 1.52845 | train_accuracy: 0.415   | test_accuracy: 0.41438 |  0:48:03s\n",
            "epoch 75 | loss: 1.52834 | train_accuracy: 0.41615 | test_accuracy: 0.41535 |  0:48:40s\n",
            "epoch 76 | loss: 1.52753 | train_accuracy: 0.41613 | test_accuracy: 0.41595 |  0:49:19s\n",
            "epoch 77 | loss: 1.52989 | train_accuracy: 0.41552 | test_accuracy: 0.4141  |  0:49:57s\n",
            "epoch 78 | loss: 1.52865 | train_accuracy: 0.41497 | test_accuracy: 0.4124  |  0:50:33s\n",
            "epoch 79 | loss: 1.52803 | train_accuracy: 0.41594 | test_accuracy: 0.41593 |  0:51:13s\n",
            "epoch 80 | loss: 1.52642 | train_accuracy: 0.41511 | test_accuracy: 0.41228 |  0:51:51s\n",
            "epoch 81 | loss: 1.52666 | train_accuracy: 0.41601 | test_accuracy: 0.41421 |  0:52:30s\n",
            "epoch 82 | loss: 1.52593 | train_accuracy: 0.41677 | test_accuracy: 0.41587 |  0:53:07s\n",
            "epoch 83 | loss: 1.52503 | train_accuracy: 0.41601 | test_accuracy: 0.41485 |  0:53:43s\n",
            "epoch 84 | loss: 1.5238  | train_accuracy: 0.41693 | test_accuracy: 0.41604 |  0:54:21s\n",
            "epoch 85 | loss: 1.52358 | train_accuracy: 0.41646 | test_accuracy: 0.41532 |  0:54:58s\n",
            "epoch 86 | loss: 1.52694 | train_accuracy: 0.41501 | test_accuracy: 0.41341 |  0:55:35s\n",
            "epoch 87 | loss: 1.52446 | train_accuracy: 0.41626 | test_accuracy: 0.41488 |  0:56:10s\n",
            "epoch 88 | loss: 1.52343 | train_accuracy: 0.41459 | test_accuracy: 0.41355 |  0:56:48s\n",
            "epoch 89 | loss: 1.52561 | train_accuracy: 0.41755 | test_accuracy: 0.41661 |  0:57:24s\n",
            "epoch 90 | loss: 1.52323 | train_accuracy: 0.4182  | test_accuracy: 0.41546 |  0:58:01s\n",
            "epoch 91 | loss: 1.52155 | train_accuracy: 0.41721 | test_accuracy: 0.41567 |  0:58:37s\n",
            "epoch 92 | loss: 1.52264 | train_accuracy: 0.41732 | test_accuracy: 0.41622 |  0:59:14s\n",
            "epoch 93 | loss: 1.52132 | train_accuracy: 0.4179  | test_accuracy: 0.41589 |  0:59:52s\n",
            "epoch 94 | loss: 1.52022 | train_accuracy: 0.41612 | test_accuracy: 0.41326 |  1:00:29s\n",
            "epoch 95 | loss: 1.52052 | train_accuracy: 0.41768 | test_accuracy: 0.41683 |  1:01:05s\n",
            "epoch 96 | loss: 1.52178 | train_accuracy: 0.41823 | test_accuracy: 0.41631 |  1:01:42s\n",
            "epoch 97 | loss: 1.52075 | train_accuracy: 0.41566 | test_accuracy: 0.41579 |  1:02:17s\n",
            "epoch 98 | loss: 1.52367 | train_accuracy: 0.41739 | test_accuracy: 0.41664 |  1:02:54s\n",
            "epoch 99 | loss: 1.52059 | train_accuracy: 0.41825 | test_accuracy: 0.41626 |  1:03:30s\n",
            "epoch 100| loss: 1.52028 | train_accuracy: 0.41871 | test_accuracy: 0.41819 |  1:04:07s\n",
            "epoch 101| loss: 1.51979 | train_accuracy: 0.41549 | test_accuracy: 0.41483 |  1:04:43s\n",
            "epoch 102| loss: 1.51934 | train_accuracy: 0.41787 | test_accuracy: 0.41562 |  1:05:19s\n",
            "epoch 103| loss: 1.51904 | train_accuracy: 0.41948 | test_accuracy: 0.41796 |  1:05:55s\n",
            "epoch 104| loss: 1.51796 | train_accuracy: 0.41817 | test_accuracy: 0.41672 |  1:06:31s\n",
            "epoch 105| loss: 1.51693 | train_accuracy: 0.41764 | test_accuracy: 0.41667 |  1:07:07s\n",
            "epoch 106| loss: 1.51929 | train_accuracy: 0.41756 | test_accuracy: 0.41633 |  1:07:43s\n",
            "epoch 107| loss: 1.51818 | train_accuracy: 0.41651 | test_accuracy: 0.41361 |  1:08:20s\n",
            "epoch 108| loss: 1.52243 | train_accuracy: 0.41653 | test_accuracy: 0.41619 |  1:08:58s\n",
            "epoch 109| loss: 1.51996 | train_accuracy: 0.41844 | test_accuracy: 0.41672 |  1:09:34s\n",
            "epoch 110| loss: 1.51904 | train_accuracy: 0.4182  | test_accuracy: 0.41708 |  1:10:11s\n",
            "epoch 111| loss: 1.52534 | train_accuracy: 0.4166  | test_accuracy: 0.41669 |  1:10:48s\n",
            "epoch 112| loss: 1.52239 | train_accuracy: 0.41794 | test_accuracy: 0.41532 |  1:11:25s\n",
            "epoch 113| loss: 1.52215 | train_accuracy: 0.41581 | test_accuracy: 0.41438 |  1:12:02s\n",
            "epoch 114| loss: 1.52167 | train_accuracy: 0.39724 | test_accuracy: 0.39544 |  1:12:38s\n",
            "epoch 115| loss: 1.53782 | train_accuracy: 0.41013 | test_accuracy: 0.40805 |  1:13:14s\n",
            "epoch 116| loss: 1.54082 | train_accuracy: 0.41244 | test_accuracy: 0.41331 |  1:13:51s\n",
            "epoch 117| loss: 1.53996 | train_accuracy: 0.40904 | test_accuracy: 0.40786 |  1:14:29s\n",
            "epoch 118| loss: 1.55033 | train_accuracy: 0.40799 | test_accuracy: 0.40713 |  1:15:05s\n",
            "epoch 119| loss: 1.54186 | train_accuracy: 0.4133  | test_accuracy: 0.41157 |  1:15:42s\n",
            "epoch 120| loss: 1.53653 | train_accuracy: 0.41336 | test_accuracy: 0.41421 |  1:16:19s\n",
            "epoch 121| loss: 1.5334  | train_accuracy: 0.41383 | test_accuracy: 0.41444 |  1:16:56s\n",
            "epoch 122| loss: 1.53161 | train_accuracy: 0.41469 | test_accuracy: 0.41421 |  1:17:32s\n",
            "epoch 123| loss: 1.52915 | train_accuracy: 0.41537 | test_accuracy: 0.41458 |  1:18:09s\n",
            "epoch 124| loss: 1.52927 | train_accuracy: 0.41591 | test_accuracy: 0.41678 |  1:18:46s\n",
            "epoch 125| loss: 1.53862 | train_accuracy: 0.41512 | test_accuracy: 0.4132  |  1:19:24s\n",
            "epoch 126| loss: 1.54166 | train_accuracy: 0.41055 | test_accuracy: 0.41138 |  1:20:03s\n",
            "epoch 127| loss: 1.5352  | train_accuracy: 0.41431 | test_accuracy: 0.41418 |  1:20:42s\n",
            "epoch 128| loss: 1.52957 | train_accuracy: 0.41272 | test_accuracy: 0.41361 |  1:21:20s\n",
            "epoch 129| loss: 1.52754 | train_accuracy: 0.41507 | test_accuracy: 0.41493 |  1:21:59s\n",
            "epoch 130| loss: 1.5253  | train_accuracy: 0.41602 | test_accuracy: 0.41535 |  1:22:38s\n",
            "epoch 131| loss: 1.52698 | train_accuracy: 0.41732 | test_accuracy: 0.41633 |  1:23:17s\n",
            "epoch 132| loss: 1.5281  | train_accuracy: 0.41445 | test_accuracy: 0.41399 |  1:24:01s\n",
            "epoch 133| loss: 1.52937 | train_accuracy: 0.41489 | test_accuracy: 0.41446 |  1:24:41s\n",
            "epoch 134| loss: 1.52723 | train_accuracy: 0.41485 | test_accuracy: 0.4141  |  1:25:19s\n",
            "epoch 135| loss: 1.5326  | train_accuracy: 0.41359 | test_accuracy: 0.41275 |  1:25:56s\n",
            "epoch 136| loss: 1.53323 | train_accuracy: 0.41218 | test_accuracy: 0.41193 |  1:26:34s\n",
            "epoch 137| loss: 1.52725 | train_accuracy: 0.41381 | test_accuracy: 0.41331 |  1:27:12s\n",
            "epoch 138| loss: 1.5226  | train_accuracy: 0.41837 | test_accuracy: 0.41768 |  1:27:53s\n",
            "epoch 139| loss: 1.52115 | train_accuracy: 0.41794 | test_accuracy: 0.41725 |  1:28:33s\n",
            "epoch 140| loss: 1.51933 | train_accuracy: 0.4192  | test_accuracy: 0.41763 |  1:29:14s\n",
            "epoch 141| loss: 1.51953 | train_accuracy: 0.41947 | test_accuracy: 0.41793 |  1:29:57s\n",
            "epoch 142| loss: 1.51763 | train_accuracy: 0.41829 | test_accuracy: 0.4163  |  1:30:39s\n",
            "epoch 143| loss: 1.5176  | train_accuracy: 0.41827 | test_accuracy: 0.41684 |  1:31:17s\n",
            "epoch 144| loss: 1.51668 | train_accuracy: 0.42003 | test_accuracy: 0.419   |  1:31:55s\n",
            "epoch 145| loss: 1.51632 | train_accuracy: 0.41868 | test_accuracy: 0.41787 |  1:32:33s\n",
            "epoch 146| loss: 1.51887 | train_accuracy: 0.41835 | test_accuracy: 0.41727 |  1:33:12s\n",
            "epoch 147| loss: 1.5197  | train_accuracy: 0.4187  | test_accuracy: 0.41768 |  1:33:53s\n",
            "epoch 148| loss: 1.52483 | train_accuracy: 0.4115  | test_accuracy: 0.41022 |  1:34:40s\n",
            "epoch 149| loss: 1.53252 | train_accuracy: 0.4141  | test_accuracy: 0.41394 |  1:35:20s\n",
            "epoch 150| loss: 1.5267  | train_accuracy: 0.41598 | test_accuracy: 0.4143  |  1:35:58s\n",
            "epoch 151| loss: 1.52182 | train_accuracy: 0.41755 | test_accuracy: 0.41662 |  1:36:36s\n",
            "epoch 152| loss: 1.51895 | train_accuracy: 0.41886 | test_accuracy: 0.41716 |  1:37:14s\n",
            "epoch 153| loss: 1.51812 | train_accuracy: 0.41971 | test_accuracy: 0.4194  |  1:37:53s\n",
            "epoch 154| loss: 1.51718 | train_accuracy: 0.41892 | test_accuracy: 0.41659 |  1:38:31s\n",
            "epoch 155| loss: 1.51654 | train_accuracy: 0.41977 | test_accuracy: 0.41867 |  1:39:10s\n",
            "epoch 156| loss: 1.51488 | train_accuracy: 0.41903 | test_accuracy: 0.41633 |  1:39:49s\n",
            "epoch 157| loss: 1.51435 | train_accuracy: 0.41943 | test_accuracy: 0.41812 |  1:40:29s\n",
            "epoch 158| loss: 1.51492 | train_accuracy: 0.42002 | test_accuracy: 0.41755 |  1:41:07s\n",
            "epoch 159| loss: 1.5138  | train_accuracy: 0.42011 | test_accuracy: 0.41829 |  1:41:46s\n",
            "epoch 160| loss: 1.51371 | train_accuracy: 0.41613 | test_accuracy: 0.41306 |  1:42:28s\n",
            "epoch 161| loss: 1.51742 | train_accuracy: 0.4166  | test_accuracy: 0.41592 |  1:43:06s\n",
            "epoch 162| loss: 1.51585 | train_accuracy: 0.41887 | test_accuracy: 0.41725 |  1:43:46s\n",
            "epoch 163| loss: 1.51885 | train_accuracy: 0.41072 | test_accuracy: 0.41075 |  1:44:25s\n",
            "epoch 164| loss: 1.52456 | train_accuracy: 0.41467 | test_accuracy: 0.41444 |  1:45:03s\n",
            "epoch 165| loss: 1.52099 | train_accuracy: 0.41704 | test_accuracy: 0.41644 |  1:45:41s\n",
            "epoch 166| loss: 1.52141 | train_accuracy: 0.41123 | test_accuracy: 0.41    |  1:46:20s\n",
            "epoch 167| loss: 1.52673 | train_accuracy: 0.41764 | test_accuracy: 0.41691 |  1:46:59s\n",
            "epoch 168| loss: 1.51845 | train_accuracy: 0.41893 | test_accuracy: 0.41863 |  1:47:36s\n",
            "epoch 169| loss: 1.52    | train_accuracy: 0.41693 | test_accuracy: 0.41468 |  1:48:14s\n",
            "epoch 170| loss: 1.52042 | train_accuracy: 0.41501 | test_accuracy: 0.41154 |  1:48:52s\n",
            "epoch 171| loss: 1.5186  | train_accuracy: 0.41679 | test_accuracy: 0.41551 |  1:49:30s\n",
            "epoch 172| loss: 1.51898 | train_accuracy: 0.41693 | test_accuracy: 0.4168  |  1:50:07s\n",
            "epoch 173| loss: 1.51685 | train_accuracy: 0.41997 | test_accuracy: 0.41821 |  1:50:45s\n",
            "epoch 174| loss: 1.51633 | train_accuracy: 0.41793 | test_accuracy: 0.41661 |  1:51:24s\n",
            "epoch 175| loss: 1.52802 | train_accuracy: 0.41646 | test_accuracy: 0.41608 |  1:52:02s\n",
            "epoch 176| loss: 1.52544 | train_accuracy: 0.41444 | test_accuracy: 0.41336 |  1:52:39s\n",
            "epoch 177| loss: 1.5222  | train_accuracy: 0.41738 | test_accuracy: 0.41727 |  1:53:17s\n",
            "epoch 178| loss: 1.52239 | train_accuracy: 0.41827 | test_accuracy: 0.41874 |  1:53:55s\n",
            "epoch 179| loss: 1.52132 | train_accuracy: 0.41546 | test_accuracy: 0.41433 |  1:54:34s\n",
            "epoch 180| loss: 1.52319 | train_accuracy: 0.41228 | test_accuracy: 0.41069 |  1:55:12s\n",
            "epoch 181| loss: 1.5228  | train_accuracy: 0.41869 | test_accuracy: 0.41705 |  1:55:50s\n",
            "epoch 182| loss: 1.52422 | train_accuracy: 0.41549 | test_accuracy: 0.41319 |  1:56:28s\n",
            "epoch 183| loss: 1.52109 | train_accuracy: 0.41892 | test_accuracy: 0.41664 |  1:57:07s\n",
            "epoch 184| loss: 1.51958 | train_accuracy: 0.41953 | test_accuracy: 0.41681 |  1:57:45s\n",
            "epoch 185| loss: 1.51731 | train_accuracy: 0.42018 | test_accuracy: 0.4186  |  1:58:23s\n",
            "epoch 186| loss: 1.51533 | train_accuracy: 0.41995 | test_accuracy: 0.41863 |  1:59:03s\n",
            "epoch 187| loss: 1.5167  | train_accuracy: 0.42029 | test_accuracy: 0.41881 |  1:59:40s\n",
            "epoch 188| loss: 1.51842 | train_accuracy: 0.41835 | test_accuracy: 0.41791 |  2:00:19s\n",
            "epoch 189| loss: 1.51521 | train_accuracy: 0.42052 | test_accuracy: 0.41859 |  2:00:56s\n",
            "epoch 190| loss: 1.51373 | train_accuracy: 0.42089 | test_accuracy: 0.41923 |  2:01:35s\n",
            "epoch 191| loss: 1.51329 | train_accuracy: 0.42038 | test_accuracy: 0.41787 |  2:02:14s\n",
            "epoch 192| loss: 1.51492 | train_accuracy: 0.4207  | test_accuracy: 0.41936 |  2:02:52s\n",
            "epoch 193| loss: 1.5171  | train_accuracy: 0.41892 | test_accuracy: 0.41612 |  2:03:30s\n",
            "epoch 194| loss: 1.51659 | train_accuracy: 0.41865 | test_accuracy: 0.41609 |  2:04:08s\n",
            "epoch 195| loss: 1.51536 | train_accuracy: 0.4207  | test_accuracy: 0.41928 |  2:04:51s\n",
            "epoch 196| loss: 1.51234 | train_accuracy: 0.42013 | test_accuracy: 0.41694 |  2:05:31s\n",
            "epoch 197| loss: 1.51139 | train_accuracy: 0.42062 | test_accuracy: 0.41863 |  2:06:10s\n",
            "epoch 198| loss: 1.51161 | train_accuracy: 0.42027 | test_accuracy: 0.41794 |  2:06:48s\n",
            "epoch 199| loss: 1.51017 | train_accuracy: 0.42275 | test_accuracy: 0.41929 |  2:07:30s\n",
            "Stop training because you reached max_epochs = 200 with best_epoch = 153 and best_test_accuracy = 0.4194\n",
            "Best weights from best epoch are automatically used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUTfqC6fdeec"
      },
      "source": [
        "[\n",
        "    {\n",
        "        epochs=200,\n",
        "        batch_size=1000,\n",
        "        acc=42,\n",
        "        test_acc=42,\n",
        "    },\n",
        "    {\n",
        "        n_d=12,\n",
        "        n_shared=4,\n",
        "        epochs=200,\n",
        "        batch_size=5000,\n",
        "        n_steps=3,\n",
        "        acc=42,\n",
        "        test_acc=42,\n",
        "    }\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}